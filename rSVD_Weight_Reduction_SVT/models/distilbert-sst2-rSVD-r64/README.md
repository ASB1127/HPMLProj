# DistilBERT SST-2 â€” rSVD Low-Rank Model (rank=64)

This model fine-tunes **DistilBERT** on the **SST-2 sentiment classification** task using
**Randomized SVD (rSVD)** to factorize the Q/K/V projection matrices in every transformer
attention layer.

The goal is to measure **memory savings**, **FLOPs reductions**, and **accuracy retention**
as the rank is varied.

---

## ðŸ”§ rSVD Method Summary

- Base model: `distilbert-base-uncased`
- Decomposition applied to: **Q, K, V attention projections**
- rSVD Rank: **64**
- Trainable params: drastically reduced vs full fine-tuning
- Architecture modified using 3-parameter low-rank factors: **A**, **C**, **B**

---

## ðŸ“Š Training Configuration

| Setting | Value |
|--------|-------|
| Rank | 64 |
| Learning Rate | 0.0002 |
| Num Epochs | 10 |
| Batch Size | 32 |
| Dataset | GLUE SST-2 |
| Optimizer | AdamW |
| Scheduler | Constant LR |

---

## ðŸ§® Performance Metrics

| Metric | Value |
|--------|--------|
| FLOPs per training step | 772,515,102,720 |
| FLOPs per epoch | 1,626,144,291,225,600 |
| Peak GPU Memory (bytes) | 1501560832 |
| Validation Accuracy | 0.8738532110091743 |

*(Metrics are computed automatically during training and profiling.)*

---

## ðŸš€ Usage Example

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model_id = "AmitBal/distilbert-sst2-rSVD-r64"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSequenceClassification.from_pretrained(model_id)

text = "This movie was absolutely fantastic!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

print(outputs.logits)
```
        